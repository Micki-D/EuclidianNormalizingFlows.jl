{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise \n",
    "using EuclidianNormalizingFlows\n",
    "\n",
    "using BAT\n",
    "using Distributions\n",
    "using Optimisers\n",
    "using FunctionChains\n",
    "using ArraysOfArrays\n",
    "using LinearAlgebra\n",
    "using ValueShapes\n",
    "using StatsBase\n",
    "using FileIO\n",
    "using JLD2\n",
    "using CUDA\n",
    "using CUDAKernels\n",
    "using KernelAbstractions\n",
    "using Flux\n",
    "using PyPlot\n",
    "\n",
    "n_smpls = 10^5\n",
    "n_dims = 200\n",
    "n_modes = 10\n",
    "\n",
    "mvns = [MvNormal(10 .*rand(n_dims), 0.5 * abs(randn(1)[1]) .* I(n_dims)) for i in 1:n_modes]\n",
    "d = MixtureModel(mvns)\n",
    "importance_density = MvNormal(zeros(n_dims), I)\n",
    "wanna_use_GPU = true\n",
    "_device = wanna_use_GPU ? KernelAbstractions.get_device(CUDA.rand(10)) : KernelAbstractions.get_device(rand(10))\n",
    "samples = bat_sample(d, BAT.IIDSampling(nsamples=n_smpls)).result;\n",
    "smpls_flat = flatview(unshaped.(samples.v));\n",
    "samples_nested = wanna_use_GPU ? nestedview(gpu(smpls_flat)) : nestedview(smpls_flat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatches = 10\n",
    "nepochs = 100\n",
    "K = 40\n",
    "\n",
    "blocks = get_flow_musketeer(n_dims,_device,K)\n",
    "#lr=range(5f-3, 5f-4, length=length(blocks.fs))\n",
    "lr = fill(3f-3, length(blocks.fs))\n",
    "trained_blocks = Function[ScaleShiftNorm(_device)]\n",
    "hists = Vector[]\n",
    "#smpls_transformed = samples_nested;\n",
    "smpls_train = samples_nested;\n",
    "\n",
    "@time begin\n",
    "    #smpls_train = nestedview(smpls_flat)\n",
    "    smpls_train = nestedview(trained_blocks[1](smpls_flat))\n",
    "    \n",
    "    \n",
    "    for i in 1:n_dims#Int(n_dims/2)#length(blocks.fs)\n",
    "        #println(\"+++ Starting round $i\")\n",
    "        #nbatches= round(Int,nbatches * 1.16)\n",
    "            \n",
    "        if i%50==0 \n",
    "            println(\"+++ Starting round $i\")\n",
    "        end\n",
    "        \n",
    "        r = optimize_whitening(smpls_train, \n",
    "            blocks.fs[1+i],\n",
    "            Optimisers.Adam(lr[i]),\n",
    "            nbatches=nbatches,\n",
    "            nepochs=nepochs, \n",
    "            shuffle_samples =false)\n",
    "            \n",
    "        trained_trafo = r.result\n",
    "        push!(trained_blocks, trained_trafo)\n",
    "        push!(hists, r.negll_history)\n",
    "        smpls_train = nestedview(trained_trafo(flatview(smpls_train)))\n",
    "    end\n",
    "end\n",
    "\n",
    "push!(trained_blocks, ScaleShiftNorm(_device))\n",
    "trained_flow = fchain(trained_blocks)\n",
    "smpls_transformed, ladj_trafo = EuclidianNormalizingFlows.with_logabsdet_jacobian(trained_flow, smpls_flat)\n",
    "\n",
    "smpls_transformed = cpu(smpls_transformed)\n",
    "smpls_flat_cpu = cpu(smpls_flat)\n",
    "\n",
    "integral, variance = ghm_integration(smpls_transformed, samples.logd, vec(cpu(ladj_trafo)), importance_density)\n",
    "\n",
    "@show integral\n",
    "@show variance\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "ax[1].hist2d(smpls_flat_cpu[1,:], smpls_flat_cpu[2,:], [100,100], cmap=\"inferno\")\n",
    "ax[1].set_xlim([minimum(smpls_flat_cpu[1,:]), maximum(smpls_flat_cpu[1,:])])\n",
    "ax[1].set_ylim([minimum(smpls_flat_cpu[2,:]), maximum(smpls_flat_cpu[2,:])])\n",
    "ax[2].hist2d(smpls_transformed[1,:], smpls_transformed[2,:], [100,100], cmap=\"inferno\")\n",
    "ax[2].set_xlim([-3, 3])\n",
    "ax[2].set_ylim([-3, 3])\n",
    "\n",
    "\n",
    "\n",
    "for i in round.(Integer, range(1,size(cpu(smpls_flat),1), 4)) #size(cpu(smpls_flat),1)\n",
    "    fig, ax = plt.subplots(1,2, figsize=(16,4))\n",
    "    bins = range(minimum(smpls_flat_cpu[i,:])-1, maximum(smpls_flat_cpu[i,:])+1, 110)\n",
    "    ax[1].hist(cpu(smpls_flat)[i,:], weights=samples.weight, bins=bins, alpha=0.3, label=\"Target Marginal\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel(\"$i\")\n",
    "    bins = range(minimum(smpls_transformed[i,:])-1, maximum(smpls_transformed[i,:])+1, 110)\n",
    "    ax[2].hist(cpu(smpls_transformed)[i,:], weights=samples.weight, bins=bins, alpha=0.3, label=\"Transformed Marginal\")\n",
    "    ax[2].hist(rand(Normal(), n_smpls),  bins=bins, alpha=0.3, label=\"Gaussian\")\n",
    "    ax[2].legend()\n",
    "    ax[2].set_xlabel(\"$i\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"Nice_MvN_200D_musketeer_23mar21st.jld2\", \n",
    "    Dict(\n",
    "    \"int\" => integral, \n",
    "    \"int_var\" => variance, \n",
    "    \"flow\" => cpu(trained_flow),\n",
    "    \"samples\" => cpu(samples),\n",
    "    \"target_dist\" => d,\n",
    "    \"nbatches\" => nbatches,\n",
    "    \"nepochs\" => nepochs,\n",
    "    \"K\" => K,\n",
    "    \"lr\" => lr,\n",
    "    \"neg_ll_hists\" => hists,\n",
    "    #\"trained_blocks\" => trained_blocks\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
